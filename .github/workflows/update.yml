name: Build updates.json (Python â€” reliable multi-source scraper)

on:
  schedule:
    - cron: "17 13 * * *"   # daily ~9:17am ET
  workflow_dispatch: {}

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Build updates.json via Python
        run: |
          python - << 'PY'
          import json, re, ssl, sys, time, os, html, urllib.parse, urllib.request

          UA = ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/124.0.0.0 Safari/537.36")

          os.makedirs("debug", exist_ok=True)
          NOW = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())

          def fetch_text(url, accept="*/*", timeout=40):
            try:
              ctx = ssl.create_default_context()
              req = urllib.request.Request(url, headers={"accept": accept, "user-agent": UA})
              with urllib.request.urlopen(req, context=ctx, timeout=timeout) as r:
                raw = r.read().decode("utf-8", errors="replace")
              print(f"[fetch_text] {url} -> {len(raw)} bytes", flush=True)
              return raw
            except Exception as e:
              print(f"[fetch_text] ERROR {url}: {e}", file=sys.stderr, flush=True)
              return ""

          def strip_tags(s):
            s = re.sub(r"<script[\\s\\S]*?</script>", " ", s, flags=re.I)
            s = re.sub(r"<style[\\s\\S]*?</style>", " ", s, flags=re.I)
            s = re.sub(r"<[^>]+>", " ", s)
            s = html.unescape(s)
            return re.sub(r"\\s+", " ", s).strip()

          def extract_anchors(html_text, base):
            # returns [(abs_url, visible_text)]
            out = []
            for m in re.finditer(r'<a\\s[^>]*href="([^"]+)"[^>]*>([\\s\\S]*?)</a>', html_text, flags=re.I):
              href, label = m.group(1).strip(), m.group(2)
              if not href: 
                continue
              # normalize and clean
              href = urllib.parse.urljoin(base, href)
              label = strip_tags(label)
              if len(label) < 8:
                continue
              out.append((href, label))
            return out

          def item(_id, source, title, date, url, tags=None):
            return {
              "id": _id,
              "source": source,
              "title": (title or "").strip()[:300],
              "date": date,
              "url": url,
              "tags": (tags or [])
            }

          # ---------- NYC RULES ----------
          def pull_nyc_rules():
            base = "https://rules.cityofnewyork.us/"
            html_text = fetch_text(base)
            if not html_text: return []
            open("debug/nyc_rules_raw.html", "w", encoding="utf-8").write(html_text[:30000])

            anchors = extract_anchors(html_text, base)
            key = re.compile(r"(rule|hearing|adopted|proposed)", re.I)
            out = []
            for href, label in anchors:
              if "rules.cityofnewyork.us" not in href:
                continue
              if key.search(label) or "/rules/" in href:
                out.append(item(f"nycr-{len(out)}-{int(time.time())}", "NYC Rules",
                                label, NOW, href, ["NYC","Rules"]))
                if len(out) >= 15: break
            json.dump(out[:5], open("debug/nyc_rules_preview.json","w",encoding="utf-8"), indent=2)
            print(f"[NYC Rules] {len(out)}")
            return out

          # ---------- CITY RECORD ----------
          def pull_city_record():
            base = "https://a856-cityrecord.nyc.gov/"
            html_text = fetch_text(base)
            if not html_text: return []
            open("debug/city_record_raw.html", "w", encoding="utf-8").write(html_text[:30000])

            anchors = extract_anchors(html_text, base)
            key = re.compile(r"(DCWP|DCA|DOL|wage|employment|labor|sick|leave|retaliation|schedule|hearing)", re.I)
            out = []
            for href, label in anchors:
              if "a856-cityrecord.nyc.gov" not in href:
                continue
              if key.search(label):
                out.append(item(f"crol-{len(out)}-{int(time.time())}", "City Record",
                                label[:160], NOW, href, ["NYC","Notices"]))
                if len(out) >= 12: break
            json.dump(out[:5], open("debug/city_record_preview.json","w",encoding="utf-8"), indent=2)
            print(f"[City Record] {len(out)}")
            return out

          # ---------- NYS REGISTER ----------
          def pull_nys_register():
            base = "https://dos.ny.gov/state-register"
            html_text = fetch_text(base)
            if not html_text: return []
            open("debug/nys_register_raw.html", "w", encoding="utf-8").write(html_text[:30000])

            # combine anchors + text lines with keywords (the page often links PDFs)
            anchors = extract_anchors(html_text, base)
            lines = [s for s in re.split(r"[\\n\\.]", strip_tags(html_text)) if s]
            key = re.compile(r"(labor|employment|wage|salary|sick|leave|retaliation|schedule|min(imum)? wage)", re.I)

            out = []
            for href, label in anchors:
              if "dos.ny.gov" in href and key.search(label):
                out.append(item(f"nysr-a-{len(out)}-{int(time.time())}", "NYS Register",
                                label[:160], NOW, href, ["NYS","Rulemaking"]))
                if len(out) >= 10: break
            if len(out) < 6:
              for ln in lines:
                if key.search(ln):
                  out.append(item(f"nysr-l-{len(out)}-{int(time.time())}", "NYS Register",
                                  ln[:160], NOW, base, ["NYS","Rulemaking"]))
                  if len(out) >= 10: break
            json.dump(out[:5], open("debug/nys_register_preview.json","w",encoding="utf-8"), indent=2)
            print(f"[NYS Register] {len(out)}")
            return out

          # ---------- NYS DOL NEWS ----------
          def pull_nys_dol():
            base = "https://dol.ny.gov/news"
            html_text = fetch_text(base)
            if not html_text: return []
            open("debug/nys_dol_raw.html","w",encoding="utf-8").write(html_text[:30000])

            out = []
            for m in re.finditer(r'<a[^>]+href="(/news/[^"#]+)"[^>]*>([\\s\\S]{8,180}?)</a>', html_text, re.I):
              href = urllib.parse.urljoin("https://dol.ny.gov", m.group(1))
              label = strip_tags(m.group(2))
              if len(label) < 12: 
                continue
              out.append(item(f"nysdol-{len(out)}-{int(time.time())}", "NYS DOL",
                              label[:200], NOW, href, ["NYS","News"]))
              if len(out) >= 12: break
            json.dump(out[:5], open("debug/nys_dol_preview.json","w",encoding="utf-8"), indent=2)
            print(f"[NYS DOL] {len(out)}")
            return out

          # ---------- Aggregate ----------
          all_items = []
          for pull in (pull_nyc_rules, pull_city_record, pull_nys_register, pull_nys_dol):
            try:
              all_items += pull()
            except Exception as e:
              print("[SOURCE ERROR]", pull.__name__, e, file=sys.stderr)

          # De-dup (source, title)
          seen, out = set(), []
          for it in all_items:
            key = (it["source"], it["title"])
            if key in seen: continue
            seen.add(key); out.append(it)

          if not out:
            out = [
              {"id":"ex1","source":"NYC Council","title":"(Example) Amend Safe and Sick Time Act",
               "date":"2025-09-20","url":"https://council.nyc.gov/legislation/","tags":["NYC","Leave"]},
              {"id":"ex2","source":"NYC Rules","title":"(Example) Proposed payroll record rule",
               "date":"2025-09-18","url":"https://rules.cityofnewyork.us/","tags":["NYC","Records"]}
            ]

          try: out.sort(key=lambda x: x.get("date",""), reverse=True)
          except: pass

          with open("updates.json","w",encoding="utf-8") as f:
            json.dump(out, f, indent=2)
          print(f"[write] updates.json with {len(out)} items", flush=True)
          PY

      - name: Commit updates.json + debug if changed
        run: |
          if [ -n "$(git status --porcelain)" ]; then
            git config user.name "github-actions[bot]"
            git config user.email "github-actions[bot]@users.noreply.github.com"
            git add updates.json debug/*
            git commit -m "chore: update updates.json (reliable multi-source)"
            git push
          else
            echo "No changes to commit."
          fi

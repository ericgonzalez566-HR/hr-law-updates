name: Update updates.json (NYC DCWP Press/News â€” wide match)

on:
  schedule:
    - cron: "29 13 * * *"   # daily ~9:29am ET
  workflow_dispatch: {}      # run anytime

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Pull DCWP press/news and merge into updates.json
        run: |
          python - << 'PY'
          import json, re, ssl, sys, time, os, html, pathlib, urllib.parse, urllib.request

          UA = ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/124.0.0.0 Safari/537.36")
          NOW = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
          os.makedirs("debug", exist_ok=True)

          def fetch_text(url, timeout=40):
            try:
              ctx = ssl.create_default_context()
              req = urllib.request.Request(url, headers={"user-agent": UA})
              with urllib.request.urlopen(req, context=ctx, timeout=timeout) as r:
                return r.read().decode("utf-8", errors="replace")
            except Exception as e:
              print("[fetch_text ERROR]", url, e, file=sys.stderr)
              return ""

          def strip_tags(s):
            s = re.sub(r"<script[\\s\\S]*?</script>", " ", s, flags=re.I)
            s = re.sub(r"<style[\\s\\S]*?</style>", " ", s, flags=re.I)
            s = re.sub(r"<[^>]+>", " ", s)
            return re.sub(r"\\s+", " ", html.unescape(s)).strip()

          def extract_links(html_text, base):
            out = []
            for m in re.finditer(r'(<a\\s[^>]*href="([^"]+)"[^>]*>)([\\s\\S]*?)</a>', html_text, flags=re.I):
              tag, href, inner = m.group(1), m.group(2).strip(), m.group(3)
              href = urllib.parse.urljoin(base, href)
              text = strip_tags(inner) or re.search(r'aria-label="([^"]+)"', tag or "", flags=re.I)
              text = text if isinstance(text, str) else (text.group(1) if text else "")
              text = text.strip()
              if len(text) < 8: 
                continue
              out.append((href, text))
            return out

          def pull_dcwp():
            pages = [
              "https://www.nyc.gov/site/dcwp/about/press-releases.page",   # DCWP press
              "https://www.nyc.gov/site/dcwp/about/news.page",             # DCWP news
              "https://www.nyc.gov/site/dca/media/press-releases.page",    # legacy DCA press
            ]
            want = re.compile(r"(press|news|enforcement|settlement|laws?|workers?|wage|fair workweek|salary|retaliation)", re.I)
            keep_path = re.compile(r"/(dcwp|dca)/", re.I)

            items = []
            for url in pages:
              html_text = fetch_text(url)
              open(f"debug/dcwp_{url.split('/')[-2]}_raw.html","w",encoding="utf-8").write(html_text[:60000])
              for href, label in extract_links(html_text, url):
                if "nyc.gov" not in href: 
                  continue
                if not keep_path.search(href): 
                  continue
                if not (("press" in href.lower()) or ("news" in href.lower()) or want.search(label)):
                  continue
                items.append({
                  "id": f"dcwp-{abs(hash(href))}",
                  "source": "NYC DCWP",
                  "title": label[:200],
                  "date": NOW,
                  "url": href,
                  "tags": ["NYC","DCWP","Press"]
                })
                if len(items) >= 40:
                  break
            json.dump(items[:8], open("debug/dcwp_preview.json","w",encoding="utf-8"), indent=2)
            print(f"[DCWP] scraped {len(items)} links")
            return items

          # Load existing updates.json (if any)
          existing = []
          p = pathlib.Path("updates.json")
          if p.exists():
            try:
              existing = json.loads(p.read_text(encoding="utf-8"))
              if not isinstance(existing, list):
                existing = []
            except Exception:
              existing = []

          # Merge by URL (dedupe)
          current_urls = { (it.get("url") or "").strip() for it in existing if isinstance(it, dict) }
          fresh = [it for it in pull_dcwp() if (it["url"] not in current_urls)]
          combined = fresh + existing
          seen, dedup = set(), []
          for it in combined:
            u = (it.get("url") or "").strip()
            if not u or u in seen: 
              continue
            seen.add(u); dedup.append(it)

          # keep file size reasonable
          dedup = dedup[:250]

          with open("updates.json","w",encoding="utf-8") as f:
            json.dump(dedup, f, indent=2)
          print(f"[write] updates.json now has {len(dedup)} items (added {len(fresh)})")
          PY

      - name: Commit updates.json if changed
        run: |
          if [ -n "$(git status --porcelain)" ]; then
            git config user.name "github-actions[bot]"
            git config user.email "github-actions[bot]@users.noreply.github.com"
            git add updates.json debug/*
            git commit -m "chore: update updates.json (DCWP wide match)"
            git push
          else
            echo "No changes to commit."
          fi
